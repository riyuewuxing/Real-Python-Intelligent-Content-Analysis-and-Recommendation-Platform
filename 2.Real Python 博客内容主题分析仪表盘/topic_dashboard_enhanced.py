import pandas as pd
import streamlit as st
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import numpy as np
from collections import Counter
import warnings
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
warnings.filterwarnings('ignore')

# ===============================
# é¡µé¢é…ç½®ä¸æ ·å¼è®¾ç½®
# ===============================

st.set_page_config(
    page_title="Real Python åšå®¢å†…å®¹ä¸»é¢˜åˆ†æä»ªè¡¨ç›˜",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å°è¯•è®¾ç½®æ”¯æŒä¸­æ–‡çš„å­—ä½“
try:
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
except Exception as e:
    st.warning(f"è®¾ç½®ä¸­æ–‡å­—ä½“å¤±è´¥: {e}ã€‚å›¾è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºã€‚è¯·ç¡®ä¿ç³»ç»Ÿå®‰è£…äº†ä¸­æ–‡å­—ä½“å¦‚SimHeiæˆ–Microsoft YaHeiã€‚")

# è®¾ç½®Seabornæ ·å¼
sns.set_palette("husl")

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-title {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        padding: 1rem;
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .topic-card {
        background: white;
        border-radius: 10px;
        padding: 1.5rem;
        margin: 1rem 0;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        border-left: 5px solid #1f77b4;
        transition: all 0.3s ease;
    }
    
    .topic-card:hover {
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.15);
        transform: translateY(-2px);
    }
    
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        margin: 0.5rem;
    }
    
    .section-header {
        color: #2c3e50;
        border-bottom: 3px solid #3498db;
        padding-bottom: 0.5rem;
        margin: 2rem 0 1rem 0;
    }
    
    .confidence-bar {
        background: linear-gradient(90deg, #ff9a9e 0%, #fecfef 50%, #fecfef 100%);
        height: 20px;
        border-radius: 10px;
        margin: 0.2rem 0;
    }
    
    .stSelectbox > label {
        font-weight: 600;
        color: #2c3e50;
    }
    
    .topic-keywords {
        background: #f8f9fa;
        padding: 0.8rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        border-left: 4px solid #17a2b8;
    }
</style>
""", unsafe_allow_html=True)

# ===============================
# æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# ===============================

@st.cache_resource
def load_nlp_resources():
    """åŠ è½½NLPèµ„æºå¹¶ä¼˜åŒ–åœç”¨è¯"""
    try:
        nltk.download('stopwords', quiet=True)
        nltk.download('wordnet', quiet=True)
        nltk.download('averaged_perceptron_tagger', quiet=True)
        
        nlp = spacy.load("en_core_web_sm")
        stop_words = set(stopwords.words('english'))
        
        # å¢å¼ºçš„é¢†åŸŸç‰¹å®šåœç”¨è¯
        domain_stopwords = {
            'python', 'code', 'example', 'tutorial', 'learn', 'use', 'using', 
            'used', 'get', 'like', 'one', 'also', 'make', 'way', 'work', 
            'time', 'need', 'want', 'see', 'know', 'take', 'real', 
            'realpython', 'article', 'post', 'blog', 'course', 'lesson',
            'chapter', 'section', 'page', 'website', 'link', 'click',
            'read', 'write', 'show', 'display', 'create', 'build'
        }
        stop_words.update(domain_stopwords)
        
        lemmatizer = WordNetLemmatizer()
        return nlp, stop_words, lemmatizer
    except Exception as e:
        st.error(f"NLPèµ„æºåŠ è½½å¤±è´¥: {e}")
        st.info("è¯·ç¡®ä¿å·²å®‰è£…spacyå’Œè‹±æ–‡æ¨¡å‹: python -m spacy download en_core_web_sm")
        st.stop()

@st.cache_data
def load_data():
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    try:
        df = pd.read_csv('../shared_data/real_python_courses_analysis.csv')
        
        required_columns = ['Title', 'Content', 'Date']
        if not all(col in df.columns for col in required_columns):
            st.error(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")
            st.stop()
        
        # æ•°æ®æ¸…ç†å’Œå¢å¼º
        df = df.dropna(subset=['Content', 'Date'])
        df = df[df['Content'].str.len() > 100]
        
        # æ›´ç¨³å¥çš„æ—¥æœŸå¤„ç†
        parsed_dates = pd.to_datetime(df['Date'], errors='coerce')
        successful_parses = parsed_dates.notna()
        num_failed = len(df) - successful_parses.sum()

        if num_failed > 0:
            st.warning(f"âš ï¸ æ—¥æœŸæ ¼å¼å¤„ç†ï¼š{num_failed}æ¡è®°å½•çš„æ—¥æœŸæ— æ³•è¢«è‡ªåŠ¨è§£æï¼Œè¿™äº›è®°å½•å°†ä½¿ç”¨ä¼°ç®—æ—¥æœŸæˆ–å¯èƒ½åœ¨æ—¶é—´è¶‹åŠ¿åˆ†æä¸­è¢«å¿½ç•¥ã€‚")
        
        df['Date'] = parsed_dates
        # å¯¹äºæ— æ³•è§£æçš„æ—¥æœŸï¼Œå¡«å……ä¸€ä¸ªå›ºå®šæ—¥æœŸï¼Œæˆ–æ ‡è®°å®ƒä»¬ä»¥ä¾¿åç»­å¤„ç†
        # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ä¸ç›´æ¥å¡«å……ï¼Œè®©åç»­çš„dtè®¿é—®å™¨è‡ªç„¶å¤„ç†NaT
        
        # åªæœ‰åœ¨Dateåˆ—æˆåŠŸè½¬æ¢ä¸ºdatetimeåæ‰å°è¯•è®¿é—®dtå±æ€§
        if pd.api.types.is_datetime64_any_dtype(df['Date']):
            df['Year'] = df['Date'].dt.year
            df['Month'] = df['Date'].dt.to_period('M')
            df['YearMonth'] = df['Date'].dt.strftime('%Y-%m')
        else:
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œåˆ›å»ºåŒ…å«NaT/NaNçš„åˆ—ä»¥é¿å…åç»­é”™è¯¯
            st.error("æ—¥æœŸåˆ—æœªèƒ½æˆåŠŸè½¬æ¢ä¸ºdatetimeæ ¼å¼ï¼Œæ—¶é—´ç›¸å…³åˆ†æå¯èƒ½ä¸å‡†ç¡®ã€‚")
            df['Year'] = np.nan
            df['Month'] = pd.NaT
            df['YearMonth'] = pd.NaT 
        
        # æ·»åŠ å†…å®¹é•¿åº¦å’Œè¯æ•°ç»Ÿè®¡
        df['content_length'] = df['Content'].str.len()
        df['word_count'] = df['Content'].str.split().str.len()
        
        return df
    except FileNotFoundError:
        st.error("æœªæ‰¾åˆ° '../shared_data/real_python_courses_analysis.csv' æ–‡ä»¶")
        st.stop()
    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        st.stop()

@st.cache_data
def enhanced_preprocess_text(text, _nlp, _stop_words, _lemmatizer):
    """å¢å¼ºçš„æ–‡æœ¬é¢„å¤„ç†"""
    # åŸºç¡€æ¸…ç†
    text = text.lower()
    
    # ç§»é™¤HTMLæ ‡ç­¾ã€ä»£ç å—å’Œç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'```[\s\S]*?```', ' CODE_BLOCK ', text)
    text = re.sub(r'`[^`]+`', ' CODE_INLINE ', text)
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    text = re.sub(r'[^a-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    # ä½¿ç”¨SpaCyè¿›è¡Œé«˜çº§å¤„ç†
    doc = _nlp(text)
    tokens = []
    
    for token in doc:
        if (token.is_alpha and 
            not token.is_stop and 
            token.text not in _stop_words and 
            len(token.text) > 2 and
            len(token.text) < 20 and
            token.pos_ in ['NOUN', 'ADJ', 'VERB']):  # åªä¿ç•™å…³é”®è¯æ€§
            tokens.append(token.lemma_)
    
    return " ".join(tokens)

@st.cache_data
def train_optimized_lda_model(texts, n_topics=7, max_features=1000, min_df=2, max_df=0.95):
    """è®­ç»ƒä¼˜åŒ–çš„LDAæ¨¡å‹"""
    # ä½¿ç”¨TF-IDFå‘é‡åŒ–ï¼ŒåŒ…å«2-gram
    vectorizer = TfidfVectorizer(
        max_df=max_df, 
        min_df=min_df, 
        max_features=max_features,
        stop_words='english',
        ngram_range=(1, 2)  # åŒ…å«1-gramå’Œ2-gram
    )
    
    tfidf_matrix = vectorizer.fit_transform(texts)
    feature_names = vectorizer.get_feature_names_out()
    
    # è®­ç»ƒLDAæ¨¡å‹
    lda_model = LatentDirichletAllocation(
        n_components=n_topics,
        learning_method='online',
        random_state=42,
        max_iter=20,
        doc_topic_prior=0.1,
        topic_word_prior=0.01,
        learning_decay=0.7
    )
    
    lda_output = lda_model.fit_transform(tfidf_matrix)
    
    return lda_model, lda_output, vectorizer, tfidf_matrix, feature_names

def get_enhanced_topic_words(model, feature_names, n_top_words=15):
    """è·å–å¢å¼ºçš„ä¸»é¢˜å…³é”®è¯å¹¶ç”Ÿæˆä¸»é¢˜åç§°"""
    topic_words = {}
    topic_names = {}
    
    # æ‰©å±•çš„ä¸»é¢˜åç§°æ˜ å°„è§„åˆ™ï¼Œå¢åŠ æ›´å¤šç»†åˆ†ä¸»é¢˜
    theme_keywords = {
        'basics_syntax': ['syntax', 'operator', 'expression', 'statement', 'indentation', 'comment', 'variable', 'type', 'value', 'literal'],
        'basics_structures': ['loop', 'for', 'while', 'if', 'else', 'condition', 'list', 'dict', 'tuple', 'set', 'comprehension', 'slice'],
        'basics_functions': ['function', 'def', 'return', 'parameter', 'argument', 'lambda', 'scope', 'closure', 'decorator', 'generator'],
        'oop': ['class', 'object', 'method', 'attribute', 'property', 'inheritance', 'polymorphism', 'encapsulation', 'instance', 'static'],
        'data_science': ['data', 'dataframe', 'pandas', 'numpy', 'polars', 'analysis', 'csv', 'datum', 'column', 'group', 'aggregate', 'polar', 'miss'],
        'web_frontend': ['html', 'css', 'javascript', 'frontend', 'dom', 'ajax', 'react', 'vue', 'angular', 'template', 'ui', 'design'],
        'web_backend': ['django', 'flask', 'fastapi', 'backend', 'api', 'rest', 'graphql', 'endpoint', 'authentication', 'authorization'],
        'web_automation': ['selenium', 'browser', 'scraping', 'crawler', 'automation', 'bot', 'headless', 'puppeteer'],
        'database': ['database', 'sql', 'mysql', 'postgresql', 'sqlite', 'query', 'table', 'record', 'orm', 'migration'],
        'devops': ['docker', 'kubernetes', 'ci', 'cd', 'pipeline', 'deployment', 'server', 'cloud', 'aws', 'azure'],
        'tools': ['tool', 'package', 'library', 'install', 'pip', 'environment', 'virtualenv', 'poetry', 'dependency', 'management'],
        'concurrency': ['async', 'await', 'thread', 'process', 'multiprocessing', 'concurrency', 'parallel', 'locking', 'queue'],
        'testing': ['test', 'unittest', 'pytest', 'mock', 'coverage', 'tdd', 'integration', 'fixture', 'assert'],
        'file_io': ['file', 'directory', 'path', 'read', 'write', 'open', 'os', 'shutil', 'json', 'csv', 'serialization'],
        'algorithms': ['algorithm', 'structure', 'sort', 'search', 'tree', 'graph', 'recursion', 'dynamic', 'complexity', 'bigo'],
        'multimedia': ['video', 'audio', 'image', 'media', 'ffmpeg', 'opencv', 'processing', 'stream', 'recognition']
    }
    
    # ä¸»é¢˜ç±»åˆ«åˆ°å‹å¥½åç§°çš„æ˜ å°„
    theme_map = {
        'basics_syntax': 'Pythonè¯­æ³•åŸºç¡€',
        'basics_structures': 'æ•°æ®ç»“æ„ä¸æ§åˆ¶æµ',
        'basics_functions': 'å‡½æ•°ä¸ä½œç”¨åŸŸ',
        'oop': 'é¢å‘å¯¹è±¡ç¼–ç¨‹',
        'data_science': 'æ•°æ®åˆ†æä¸ç§‘å­¦è®¡ç®—',
        'web_frontend': 'Webå‰ç«¯æŠ€æœ¯',
        'web_backend': 'Webåç«¯å¼€å‘',
        'web_automation': 'Webè‡ªåŠ¨åŒ–ä¸çˆ¬è™«',
        'database': 'æ•°æ®åº“æ“ä½œ',
        'devops': 'DevOpsä¸éƒ¨ç½²',
        'tools': 'å¼€å‘å·¥å…·ä¸ç¯å¢ƒ',
        'concurrency': 'å¹¶å‘ä¸å¹¶è¡Œç¼–ç¨‹',
        'testing': 'æµ‹è¯•ä¸è´¨é‡ä¿éšœ',
        'file_io': 'æ–‡ä»¶ä¸IOæ“ä½œ',
        'algorithms': 'ç®—æ³•ä¸æ•°æ®ç»“æ„',
        'multimedia': 'å¤šåª’ä½“å¤„ç†'
    }
    
    # ç”¨äºæ£€æµ‹é‡å¤ä¸»é¢˜åç§°çš„å­—å…¸
    used_names = set()
    
    for topic_idx, topic in enumerate(model.components_):
        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_features_ind]
        topic_words[f"ä¸»é¢˜ {topic_idx + 1}"] = top_words
        
        # è‡ªåŠ¨ç”Ÿæˆä¸»é¢˜åç§° - æ”¹è¿›åŒ¹é…é€»è¾‘
        topic_name = f"ä¸»é¢˜ {topic_idx + 1}"
        top_words_str = ' '.join(top_words[:10]).lower()  # æ‰©å±•åˆ°å‰10ä¸ªè¯
        
        # è®¡ç®—æ¯ä¸ªä¸»é¢˜ç±»åˆ«çš„åŒ¹é…åˆ†æ•°
        theme_scores = {}
        for theme, keywords in theme_keywords.items():
            score = sum(1 for keyword in keywords if keyword in top_words_str)
            if score > 0:
                theme_scores[theme] = score
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ä¸»é¢˜ç±»åˆ«
        if theme_scores:
            # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰3ä¸ªå€™é€‰ä¸»é¢˜
            sorted_themes = sorted(theme_scores.items(), key=lambda x: x[1], reverse=True)[:3]
            
            # å°è¯•æ‰¾åˆ°æœªä½¿ç”¨çš„ä¸»é¢˜åç§°
            selected_name = None
            for theme, score in sorted_themes:
                candidate_name = theme_map.get(theme, 'ç»¼åˆä¸»é¢˜')
                if candidate_name not in used_names:
                    selected_name = candidate_name
                    break
            
            # å¦‚æœæ‰€æœ‰å€™é€‰åç§°éƒ½å·²ä½¿ç”¨ï¼Œåˆ™ä½¿ç”¨æœ€åŒ¹é…çš„åç§°åŠ ä¸Šå…³é”®è¯
            if selected_name is None:
                best_theme = sorted_themes[0][0]
                base_name = theme_map.get(best_theme, 'ç»¼åˆä¸»é¢˜')
                # ä½¿ç”¨å‰ä¸¤ä¸ªå…³é”®è¯åˆ›å»ºå”¯ä¸€åç§°
                keyword_suffix = "_".join(top_words[:2])
                selected_name = f"{base_name}: {keyword_suffix}"
            else:
                used_names.add(selected_name)
            
            topic_name = f"ä¸»é¢˜ {topic_idx + 1}: {selected_name}"
        else:
            # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œæ ¹æ®å…³é”®è¯ç‰¹å¾ç”Ÿæˆæè¿°æ€§åç§°
            if any(word in top_words_str for word in ['python', 'code', 'program']):
                base_name = "Pythonç¼–ç¨‹"
            elif any(word in top_words_str for word in ['learn', 'tutorial', 'guide']):
                base_name = "å­¦ä¹ æ•™ç¨‹"
            elif any(word in top_words_str for word in ['web', 'http', 'site']):
                base_name = "Webå¼€å‘"
            elif any(word in top_words_str for word in ['data', 'analysis', 'science']):
                base_name = "æ•°æ®åˆ†æ"
            else:
                base_name = "ç»¼åˆæŠ€æœ¯"
            
            # ç¡®ä¿åç§°å”¯ä¸€
            if base_name in used_names:
                # ä½¿ç”¨å‰ä¸¤ä¸ªå…³é”®è¯åˆ›å»ºå”¯ä¸€åç§°
                keyword_suffix = "_".join(top_words[:2])
                base_name = f"{base_name}: {keyword_suffix}"
            else:
                used_names.add(base_name)
            
            topic_name = f"ä¸»é¢˜ {topic_idx + 1}: {base_name}"
        
        topic_names[f"ä¸»é¢˜ {topic_idx + 1}"] = topic_name
        
    return topic_words, topic_names

def create_enhanced_pie_chart(data, title):
    fig = go.Figure(data=[go.Pie(
        labels=data.index,
        values=data.values,
        hole=0.4,
        textinfo='label+percent',
        textposition='outside',
        marker=dict(
            colors=px.colors.qualitative.Set3,
            line=dict(color='#FFFFFF', width=2)
        ),
        hovertemplate='<b>%{label}</b><br>æ–‡ç« æ•°: %{value}<br>å æ¯”: %{percent}<extra></extra>'
    )])
    fig.update_layout(
        title={'text': title, 'x': 0.5, 'xanchor': 'center', 'font': {'size': 18, 'family': 'Arial, sans-serif'}},
        showlegend=True, height=500, margin=dict(t=100, b=50, l=50, r=50),
        font=dict(family="SimHei, Microsoft YaHei, Arial Unicode MS, sans-serif") # ä¸ºPlotlyå›¾è¡¨è®¾ç½®å­—ä½“
    )
    return fig

def create_enhanced_bar_chart(data, title, xlabel, ylabel):
    fig = go.Figure(data=[go.Bar(
        x=data.index,
        y=data.values,
        marker_color=px.colors.qualitative.Set3[:len(data)],
        text=data.values,
        textposition='outside',
        hovertemplate='<b>%{x}</b><br>' + ylabel + ': %{y}<extra></extra>'
    )])
    fig.update_layout(
        title={'text': title, 'x': 0.5, 'xanchor': 'center', 'font': {'size': 18, 'family': 'Arial, sans-serif'}},
        xaxis_title=xlabel, yaxis_title=ylabel, height=500, margin=dict(t=100, b=50, l=50, r=50),
        xaxis={'tickangle': -45},
        font=dict(family="SimHei, Microsoft YaHei, Arial Unicode MS, sans-serif") # ä¸ºPlotlyå›¾è¡¨è®¾ç½®å­—ä½“
    )
    return fig

def create_confidence_visualization(confidence_data):
    fig = go.Figure(data=[go.Bar(
        x=confidence_data.index,
        y=confidence_data.values,
        marker=dict(color=confidence_data.values, colorscale='Viridis', showscale=True, colorbar=dict(title="ç½®ä¿¡åº¦")),
        text=[f"{x:.3f}" for x in confidence_data.values],
        textposition='outside',
        hovertemplate='<b>%{x}</b><br>å¹³å‡ç½®ä¿¡åº¦: %{y:.3f}<extra></extra>'
    )])
    fig.update_layout(
        title={'text': "å„ä¸»é¢˜å¹³å‡ç½®ä¿¡åº¦åˆ†æ", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 18, 'family': 'Arial, sans-serif'}},
        xaxis_title="ä¸»é¢˜", yaxis_title="å¹³å‡ç½®ä¿¡åº¦", height=500, margin=dict(t=100, b=50, l=50, r=50),
        xaxis={'tickangle': -45},
        font=dict(family="SimHei, Microsoft YaHei, Arial Unicode MS, sans-serif") # ä¸ºPlotlyå›¾è¡¨è®¾ç½®å­—ä½“
    )
    return fig

# ===============================
# ä¸»åº”ç”¨ç¨‹åº
# ===============================

def main():
    # ä¸»æ ‡é¢˜
    st.markdown('<h1 class="main-title">ğŸ“š Real Python åšå®¢å†…å®¹ä¸»é¢˜åˆ†æä»ªè¡¨ç›˜</h1>', 
                unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; color: #7f8c8d; font-size: 1.2rem;">åŸºäºé«˜çº§NLPæŠ€æœ¯çš„æ™ºèƒ½ä¸»é¢˜å‘ç°ä¸å†…å®¹åˆ†æå¹³å°</p>', 
                unsafe_allow_html=True)
    
    # åŠ è½½èµ„æº
    with st.spinner("ğŸ”„ æ­£åœ¨åŠ è½½NLPèµ„æºå’Œæ•°æ®..."):
        nlp, stop_words, lemmatizer = load_nlp_resources()
        df = load_data()
    
    # å®‰å…¨åœ°æ˜¾ç¤ºæ—¥æœŸèŒƒå›´ - æ ¹æ®load_dataçš„ä¿®æ”¹è°ƒæ•´
    date_range_str = ""
    if 'Date' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Date']) and df['Date'].notna().any():
        try:
            min_date = df['Date'].dropna().min()
            max_date = df['Date'].dropna().max()
            if pd.notna(min_date) and pd.notna(max_date):
                 date_range_str = f"ï¼Œæ•°æ®æ—¶é—´èŒƒå›´ï¼š{min_date.strftime('%Y-%m')} è‡³ {max_date.strftime('%Y-%m')}"
        except Exception as e:
            st.warning(f"ç”Ÿæˆæ—¥æœŸèŒƒå›´å­—ç¬¦ä¸²æ—¶å‡ºé”™: {e}") # æ›´å…·ä½“çš„é”™è¯¯
            date_range_str = "ï¼Œæ—¥æœŸèŒƒå›´ä¿¡æ¯éƒ¨åˆ†å¯ç”¨"
    elif 'Date' not in df.columns or not pd.api.types.is_datetime64_any_dtype(df['Date']):
        date_range_str = "ï¼Œæ—¥æœŸä¿¡æ¯ä¸å¯ç”¨æˆ–æ ¼å¼ä¸æ­£ç¡®"

    st.success(f"âœ… æˆåŠŸåŠ è½½ {len(df)} ç¯‡æ–‡ç« {date_range_str}")
    
    # ===============================
    # ä¾§è¾¹æ  - åˆ†æè®¾ç½®æ§åˆ¶é¢æ¿
    # ===============================
    
    st.sidebar.markdown('<h2 style="color: #2c3e50;">ğŸ›ï¸ åˆ†æè®¾ç½®æ§åˆ¶é¢æ¿</h2>', unsafe_allow_html=True)
    
    # ä¸»é¢˜æ•°é‡é€‰æ‹©
    st.sidebar.markdown("### ğŸ“Š ä¸»é¢˜å»ºæ¨¡å‚æ•°")
    n_topics = st.sidebar.slider(
        "ä¸»é¢˜æ•°é‡ (æ¨è: 7ä¸ªä¸»é¢˜)", 
        min_value=3, 
        max_value=15, 
        value=7,
        help="é€‰æ‹©LDAæ¨¡å‹è¦å‘ç°çš„ä¸»é¢˜æ•°é‡ã€‚æ›´å¤šä¸»é¢˜æä¾›æ›´ç»†è‡´çš„åˆ†ç±»ï¼Œä½†å¯èƒ½é™ä½å¯è§£é‡Šæ€§ã€‚"
    )
    
    max_features = st.sidebar.slider(
        "æœ€å¤§ç‰¹å¾è¯æ•°é‡", 
        min_value=500, 
        max_value=2000, 
        value=1000,
        help="æ§åˆ¶ç”¨äºä¸»é¢˜å»ºæ¨¡çš„è¯æ±‡è¡¨å¤§å°ã€‚æ›´å¤§çš„è¯æ±‡è¡¨å¯èƒ½æä¾›æ›´ä¸°å¯Œçš„ä¸»é¢˜æè¿°ã€‚"
    )
    
    # é«˜çº§å‚æ•°è®¾ç½®
    with st.sidebar.expander("ğŸ”§ é«˜çº§å‚æ•°è®¾ç½®"):
        min_df = st.slider("æœ€å°æ–‡æ¡£é¢‘ç‡", 1, 10, 2, 
                          help="è¯è¯­è‡³å°‘åœ¨å¤šå°‘ç¯‡æ–‡æ¡£ä¸­å‡ºç°æ‰è¢«ä¿ç•™")
        max_df = st.slider("æœ€å¤§æ–‡æ¡£é¢‘ç‡", 0.8, 0.99, 0.95, 
                          help="åœ¨è¶…è¿‡æ­¤æ¯”ä¾‹æ–‡æ¡£ä¸­å‡ºç°çš„è¯è¯­å°†è¢«è¿‡æ»¤")
    
    # æ˜¾ç¤ºæ¨èå€¼
    st.sidebar.info(f"ğŸ’¡ å½“å‰è®¾ç½®å»ºè®®ï¼š\n- æ¨èä¸»é¢˜æ•°ï¼š7ä¸ª\n- æ¨èç‰¹å¾è¯æ•°ï¼š1000ä¸ª\n- åŸºäºæ•°æ®é›†å¤§å°ä¼˜åŒ–")
    
    # ===============================
    # æ–‡æœ¬é¢„å¤„ç†
    # ===============================
    
    if 'enhanced_cleaned_content' not in df.columns or df['enhanced_cleaned_content'].isnull().all():
        with st.spinner("ğŸ”„ æ­£åœ¨è¿›è¡Œé«˜çº§æ–‡æœ¬é¢„å¤„ç†..."):
            progress_bar = st.progress(0)
            cleaned_texts = []
            
            for i, content in enumerate(df['Content']):
                if pd.notna(content):
                    cleaned_text = enhanced_preprocess_text(content, nlp, stop_words, lemmatizer)
                    cleaned_texts.append(cleaned_text)
                else:
                    cleaned_texts.append("") # å¤„ç†ç©ºå†…å®¹
                progress_bar.progress((i + 1) / len(df))
            
            df['enhanced_cleaned_content'] = cleaned_texts
            progress_bar.empty()
    
    # è¿‡æ»¤å¤„ç†åçš„æ–‡æœ¬
    df_processed = df[df['enhanced_cleaned_content'].str.len() > 20].copy()
    
    if len(df_processed) == 0:
        st.error("âš ï¸ é¢„å¤„ç†åæ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹å¯ä¾›åˆ†æã€‚è¯·æ£€æŸ¥æ•°æ®æºæˆ–è°ƒæ•´é¢„å¤„ç†å‚æ•°ã€‚")
        st.stop()
    
    # ===============================
    # è®­ç»ƒLDAæ¨¡å‹
    # ===============================
    
    with st.spinner("ğŸ§  æ­£åœ¨è®­ç»ƒä¼˜åŒ–çš„LDAä¸»é¢˜æ¨¡å‹..."):
        lda_model, lda_output, vectorizer, tfidf_matrix, feature_names = train_optimized_lda_model(
            df_processed['enhanced_cleaned_content'], n_topics, max_features, min_df, max_df
        )
    
    # åˆ†é…ä¸»é¢˜
    df_processed.loc[:, 'dominant_topic'] = lda_output.argmax(axis=1)
    df_processed.loc[:, 'topic_probability'] = lda_output.max(axis=1)
    
    # è·å–ä¸»é¢˜è¯æ±‡å’Œåç§°
    top_topic_words, topic_names = get_enhanced_topic_words(lda_model, feature_names, 15)
    
    st.success("âœ… é«˜çº§ä¸»é¢˜æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    # ===============================
    # ä¸»å†…å®¹åŒºåŸŸ
    # ===============================
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ¯ ä¸»é¢˜æ¦‚è§ˆ", "ğŸ“Š åˆ†å¸ƒåˆ†æ", "ğŸ” è¯¦ç»†åˆ†æ", "ğŸ“ˆ è¶‹åŠ¿åˆ†æ", "ğŸ“‹ æ•°æ®ç»Ÿè®¡"
    ])
    
    # ===============================
    # Tab 1: ä¸»é¢˜æ¦‚è§ˆ
    # ===============================
    
    with tab1:
        st.markdown('<h2 class="section-header">ğŸ¯ æ™ºèƒ½ä¸»é¢˜å‘ç°æ¦‚è§ˆ</h2>', unsafe_allow_html=True)
        
        # åŠ¨æ€ä¸»é¢˜å¡ç‰‡å¸ƒå±€
        cols_per_row = 3
        for i in range(0, len(top_topic_words), cols_per_row):
            cols = st.columns(cols_per_row)
            
            for j, col in enumerate(cols):
                topic_idx = i + j
                if topic_idx < len(top_topic_words):
                    topic_key = list(top_topic_words.keys())[topic_idx]
                    topic_name = topic_names[topic_key]
                    words = top_topic_words[topic_key]
                    article_count = len(df_processed[df_processed['dominant_topic'] == topic_idx])
                    
                    with col:
                        # ä¸»é¢˜å¡ç‰‡
                        st.markdown(f"""
                        <div class="topic-card">
                            <h3 style="color: #2c3e50; margin: 0 0 1rem 0;">{topic_name}</h3>
                            <div class="topic-keywords">
                                <strong>æ ¸å¿ƒå…³é”®è¯ï¼š</strong><br>
                                {', '.join(words[:6])}
                            </div>
                            <p><strong>ğŸ“„ æ–‡ç« æ•°é‡ï¼š</strong> {article_count} ç¯‡</p>
                            <p><strong>ğŸ“ ç®€è¦æè¿°ï¼š</strong> æ¶µç›– {words[0]} ç›¸å…³çš„ {words[1]} æŠ€æœ¯å’Œåº”ç”¨</p>
                        </div>
                        """, unsafe_allow_html=True)
    
    # ===============================
    # Tab 2: åˆ†å¸ƒåˆ†æ
    # ===============================
    
    with tab2:
        st.markdown('<h2 class="section-header">ğŸ“Š ä¸»é¢˜åˆ†å¸ƒæ·±åº¦åˆ†æ</h2>', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # å¢å¼ºé¥¼å›¾
            topic_counts = df_processed['dominant_topic'].value_counts()
            topic_counts.index = [topic_names[f"ä¸»é¢˜ {i + 1}"] for i in topic_counts.index]
            
            st.subheader("ğŸ“ˆ å„ä¸»é¢˜æ–‡ç« åˆ†å¸ƒ")
            fig_pie = create_enhanced_pie_chart(topic_counts, "ä¸»é¢˜æ–‡ç« åˆ†å¸ƒ")
            st.plotly_chart(fig_pie, use_container_width=True)
        
        with col2:
            # å¢å¼ºæŸ±çŠ¶å›¾
            st.subheader("ğŸ“Š ä¸»é¢˜æ–‡ç« æ•°é‡ç»Ÿè®¡")
            fig_bar = create_enhanced_bar_chart(topic_counts, "ä¸»é¢˜æ–‡ç« æ•°é‡", "ä¸»é¢˜", "æ–‡ç« æ•°é‡")
            st.plotly_chart(fig_bar, use_container_width=True)
        
        # ç½®ä¿¡åº¦åˆ†æ
        st.subheader("ğŸ¯ ä¸»é¢˜ç½®ä¿¡åº¦æ·±åº¦åˆ†æ")
        avg_confidence = df_processed.groupby('dominant_topic')['topic_probability'].mean()
        avg_confidence.index = [topic_names[f"ä¸»é¢˜ {i + 1}"] for i in avg_confidence.index]
        
        fig_conf = create_confidence_visualization(avg_confidence)
        st.plotly_chart(fig_conf, use_container_width=True)
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        st.info(f"""
        ğŸ“Š **ç½®ä¿¡åº¦ç»Ÿè®¡ä¿¡æ¯:**
        - å¹³å‡ç½®ä¿¡åº¦: {df_processed['topic_probability'].mean():.3f}
        - æœ€é«˜ç½®ä¿¡åº¦: {df_processed['topic_probability'].max():.3f}
        - ä½ç½®ä¿¡åº¦æ–‡ç«  (<0.3): {len(df_processed[df_processed['topic_probability'] < 0.3])} ç¯‡
        - é«˜ç½®ä¿¡åº¦æ–‡ç«  (>0.7): {len(df_processed[df_processed['topic_probability'] > 0.7])} ç¯‡
        """)
    
    # ===============================
    # Tab 3: è¯¦ç»†åˆ†æ
    # ===============================
    
    with tab3:
        st.markdown('<h2 class="section-header">ğŸ” ä¸»é¢˜è¯¦ç»†æ·±åº¦åˆ†æ</h2>', unsafe_allow_html=True)
        
        # ä¸»é¢˜é€‰æ‹©
        selected_topic_name = st.selectbox(
            "ğŸ¯ é€‰æ‹©è¦åˆ†æçš„ä¸»é¢˜",
            options=list(topic_names.values()),
            help="é€‰æ‹©ä¸€ä¸ªä¸»é¢˜è¿›è¡Œæ·±å…¥åˆ†æ"
        )
        
        # è·å–é€‰ä¸­ä¸»é¢˜çš„ç´¢å¼•
        selected_topic_id = None
        for key, name in topic_names.items():
            if name == selected_topic_name:
                selected_topic_id = int(key.split()[-1]) - 1
                break
        
        if selected_topic_id is not None:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.subheader(f"ğŸ“„ {selected_topic_name} - ç›¸å…³æ–‡ç« åˆ—è¡¨")
                
                # è¿‡æ»¤å’Œæ’åºæ–‡ç« 
                topic_articles = df_processed[df_processed['dominant_topic'] == selected_topic_id].sort_values(
                    by='topic_probability', ascending=False
                ).head(20)
                
                # æ˜¾ç¤ºæ–‡ç« è¡¨æ ¼
                for idx, row in topic_articles.iterrows():
                    confidence = row['topic_probability']
                    confidence_color = 'green' if confidence > 0.7 else 'orange' if confidence > 0.4 else 'red'
                    
                    with st.expander(f"ğŸ“„ {row['Title']} (ç½®ä¿¡åº¦: {confidence:.3f})"):
                        col_a, col_b = st.columns([3, 1])
                        
                        with col_a:
                            # å®‰å…¨åœ°æ˜¾ç¤ºæ—¥æœŸ
                            try:
                                if pd.api.types.is_datetime64_any_dtype(df['Date']):
                                    date_str = row['Date'].strftime('%Y-%m-%d')
                                else:
                                    date_str = str(row['Date'])
                            except:
                                date_str = "æ—¥æœŸæœªçŸ¥"
                            
                            st.write(f"**ğŸ“… å‘å¸ƒæ—¥æœŸ:** {date_str}")
                            if 'URL' in row and pd.notna(row['URL']):
                                st.write(f"**ğŸ”— åŸæ–‡é“¾æ¥:** [{row['URL']}]({row['URL']})")
                            
                            # å†…å®¹é¢„è§ˆ
                            content_preview = row['Content'][:300] + "..." if len(row['Content']) > 300 else row['Content']
                            st.write(f"**ğŸ“ å†…å®¹é¢„è§ˆ:** {content_preview}")
                        
                        with col_b:
                            # ç½®ä¿¡åº¦æ¡å½¢å›¾
                            st.markdown(f"""
                            <div style="text-align: center;">
                                <strong>ç½®ä¿¡åº¦</strong><br>
                                <div style="background: #f0f0f0; border-radius: 10px; overflow: hidden; margin: 10px 0;">
                                    <div style="background: {confidence_color}; height: 20px; width: {confidence*100}%; border-radius: 10px;"></div>
                                </div>
                                <span style="color: {confidence_color}; font-weight: bold;">{confidence:.1%}</span>
                            </div>
                            """, unsafe_allow_html=True)
            
            with col2:
                st.subheader(f"â˜ï¸ {selected_topic_name} - è¯äº‘")
                
                # ç”Ÿæˆè¯äº‘
                topic_words = top_topic_words[f"ä¸»é¢˜ {selected_topic_id + 1}"]
                word_freq = {word: len(topic_words) - i for i, word in enumerate(topic_words)}
                
                try:
                    wordcloud = WordCloud(
                        width=400, 
                        height=300, 
                        background_color='white',
                        colormap='viridis',
                        max_words=30,
                        relative_scaling=0.5,
                        min_font_size=12
                    ).generate_from_frequencies(word_freq)
                    
                    fig_wc, ax_wc = plt.subplots(figsize=(8, 6))
                    ax_wc.imshow(wordcloud, interpolation='bilinear')
                    ax_wc.axis("off")
                    ax_wc.set_title(f"{selected_topic_name} å…³é”®è¯äº‘", 
                                   fontsize=14, fontweight='bold', pad=20)
                    st.pyplot(fig_wc)
                except Exception as e:
                    st.error(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {e}")
                
                # å…³é”®è¯åˆ—è¡¨
                st.subheader("ğŸ”‘ æ ¸å¿ƒå…³é”®è¯æ’åº")
                for i, word in enumerate(topic_words[:10]):
                    st.write(f"{i+1}. **{word}**")
    
    # ===============================
    # Tab 4: è¶‹åŠ¿åˆ†æ
    # ===============================
    
    with tab4:
        st.markdown('<h2 class="section-header">ğŸ“ˆ ä¸»é¢˜æ—¶é—´è¶‹åŠ¿åˆ†æ</h2>', unsafe_allow_html=True)
        
        if 'Year' in df_processed.columns and df_processed['Year'].nunique() > 1:
            # å¹´åº¦è¶‹åŠ¿
            yearly_trends = df_processed.groupby(['Year', 'dominant_topic']).size().reset_index(name='æ–‡ç« æ•°é‡')
            yearly_trends['ä¸»é¢˜åç§°'] = yearly_trends['dominant_topic'].apply(
                lambda x: topic_names[f"ä¸»é¢˜ {x + 1}"]
            )
            
            fig_trend = px.line(
                yearly_trends,
                x='Year',
                y='æ–‡ç« æ•°é‡',
                color='ä¸»é¢˜åç§°',
                title='å„ä¸»é¢˜æ–‡ç« å‘å¸ƒå¹´åº¦è¶‹åŠ¿',
                markers=True,
                height=500
            )
            fig_trend.update_layout(
                title={'x': 0.5, 'xanchor': 'center'},
                xaxis_title="å¹´ä»½",
                yaxis_title="æ–‡ç« å‘å¸ƒæ•°é‡"
            )
            st.plotly_chart(fig_trend, use_container_width=True)
            
            # æœˆåº¦çƒ­åŠ›å›¾
            if 'YearMonth' in df_processed.columns:
                st.subheader("ğŸ“… ä¸»é¢˜å‘å¸ƒæœˆåº¦çƒ­åŠ›å›¾")
                monthly_data = df_processed.groupby(['YearMonth', 'dominant_topic']).size().unstack(fill_value=0)
                monthly_data.columns = [topic_names[f"ä¸»é¢˜ {i + 1}"] for i in monthly_data.columns]
                
                fig_heatmap = px.imshow(
                    monthly_data.T,
                    title="ä¸»é¢˜æœˆåº¦å‘å¸ƒçƒ­åŠ›å›¾",
                    color_continuous_scale="Viridis",
                    height=400
                )
                fig_heatmap.update_layout(title={'x': 0.5, 'xanchor': 'center'})
                st.plotly_chart(fig_heatmap, use_container_width=True)
        else:
            st.info("ğŸ“… æ•°æ®ä¸­ç¼ºå°‘è¶³å¤Ÿçš„æ—¶é—´ä¿¡æ¯æ¥è¿›è¡Œè¶‹åŠ¿åˆ†æ")
    
    # ===============================
    # Tab 5: æ•°æ®ç»Ÿè®¡
    # ===============================
    
    with tab5:
        st.markdown('<h2 class="section-header">ğŸ“‹ ç»¼åˆæ•°æ®ç»Ÿè®¡ä¿¡æ¯</h2>', unsafe_allow_html=True)
        
        # å…³é”®æŒ‡æ ‡å¡ç‰‡
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ“„ æ€»æ–‡ç« æ•°</h3>
                <h2>{len(df_processed)}</h2>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ¯ ä¸»é¢˜æ•°é‡</h3>
                <h2>{n_topics}</h2>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ“š è¯æ±‡è¡¨å¤§å°</h3>
                <h2>{len(feature_names)}</h2>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            avg_confidence = df_processed['topic_probability'].mean()
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ² å¹³å‡ç½®ä¿¡åº¦</h3>
                <h2>{avg_confidence:.3f}</h2>
            </div>
            """, unsafe_allow_html=True)
        
        # è¯¦ç»†ç»Ÿè®¡
        st.subheader("ğŸ“Š è¯¦ç»†æ•°æ®ç»Ÿè®¡")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("### ğŸ“ˆ æ–‡ç« é•¿åº¦åˆ†æ")
            length_stats = df_processed['content_length'].describe()
            st.write(length_stats)
            
            # æ–‡ç« é•¿åº¦åˆ†å¸ƒ
            fig_length = px.histogram(
                df_processed, x='content_length', 
                title="æ–‡ç« å­—ç¬¦é•¿åº¦åˆ†å¸ƒ",
                nbins=30
            )
            st.plotly_chart(fig_length, use_container_width=True)
        
        with col2:
            st.write("### ğŸ”¤ è¯æ•°ç»Ÿè®¡åˆ†æ")
            word_stats = df_processed['word_count'].describe()
            st.write(word_stats)
            
            # è¯æ•°åˆ†å¸ƒ
            fig_words = px.histogram(
                df_processed, x='word_count', 
                title="æ–‡ç« è¯æ•°åˆ†å¸ƒ",
                nbins=30
            )
            st.plotly_chart(fig_words, use_container_width=True)
        
        # é«˜é¢‘è¯æ±‡åˆ†æ
        st.subheader("ğŸ”¤ é«˜é¢‘å…³é”®è¯ç»Ÿè®¡")
        
        all_words = []
        for text in df_processed['enhanced_cleaned_content']:
            all_words.extend(text.split())
        
        word_freq = Counter(all_words)
        top_words = word_freq.most_common(20)
        
        word_df = pd.DataFrame(top_words, columns=['è¯æ±‡', 'é¢‘æ¬¡'])
        
        fig_freq = px.bar(
            word_df, x='è¯æ±‡', y='é¢‘æ¬¡',
            title="é«˜é¢‘è¯æ±‡æ’è¡Œæ¦œ (Top 20)",
            color='é¢‘æ¬¡',
            color_continuous_scale='viridis'
        )
        fig_freq.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig_freq, use_container_width=True)
        
        # ä¸‹è½½åŒºåŸŸ
        st.subheader("ğŸ’¾ æ•°æ®å¯¼å‡º")
        
        # å‡†å¤‡ä¸‹è½½æ•°æ®
        download_df = df_processed[[
            'Title', 'Date', 'dominant_topic', 'topic_probability', 'content_length', 'word_count'
        ]].copy()
        download_df['ä¸»é¢˜åç§°'] = download_df['dominant_topic'].apply(
            lambda x: topic_names[f"ä¸»é¢˜ {x + 1}"]
        )
        download_df['ä¸»é¢˜å…³é”®è¯'] = download_df['dominant_topic'].apply(
            lambda x: ', '.join(top_topic_words[f"ä¸»é¢˜ {x + 1}"][:5])
        )
        
        # ç¡®ä¿Dateåˆ—æ˜¯å­—ç¬¦ä¸²æ ¼å¼ä»¥ä¾¿CSVå¯¼å‡º
        if 'Date' in download_df.columns and pd.api.types.is_datetime64_any_dtype(download_df['Date']):
            download_df['Date'] = download_df['Date'].dt.strftime('%Y-%m-%d %H:%M:%S').fillna('æ—¥æœŸè§£æå¤±è´¥')
        elif 'Date' in download_df.columns:
            download_df['Date'] = download_df['Date'].astype(str).fillna('æ—¥æœŸè§£æå¤±è´¥')

        csv = download_df.to_csv(index=False, encoding='utf-8-sig')
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½å®Œæ•´åˆ†æç»“æœ (CSV)",
            data=csv,
            file_name=f"real_python_topic_analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
        
        # æ˜¾ç¤ºç»“æœé¢„è§ˆ
        with st.expander("ğŸ“Š æŸ¥çœ‹åˆ†æç»“æœé¢„è§ˆ"):
            st.dataframe(download_df, use_container_width=True)

if __name__ == "__main__":
    main()
