import pandas as pd
import streamlit as st
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import numpy as np
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Real Python åšå®¢å†…å®¹ä¸»é¢˜åˆ†æ",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åŠ è½½SpaCyæ¨¡å‹å’ŒNLTKèµ„æº
@st.cache_resource
def load_nlp_resources():
    try:
        # ä¸‹è½½NLTKæ•°æ®
        import nltk
        nltk.download('stopwords', quiet=True)
        nltk.download('wordnet', quiet=True)
        nltk.download('averaged_perceptron_tagger', quiet=True)
        
        # åŠ è½½SpaCyæ¨¡å‹
        nlp = spacy.load("en_core_web_sm")
        stop_words = set(stopwords.words('english'))
        # æ·»åŠ ä¸€äº›é¢†åŸŸç›¸å…³çš„åœç”¨è¯
        stop_words.update(['python', 'code', 'example', 'tutorial', 'learn', 'use', 'using', 'used', 'get', 'like', 'one', 'also', 'make', 'way', 'work', 'time', 'need', 'want', 'see', 'know', 'take', 'real', 'realpython'])
        lemmatizer = WordNetLemmatizer()
        return nlp, stop_words, lemmatizer
    except Exception as e:
        st.error(f"NLPèµ„æºåŠ è½½å¤±è´¥: {e}")
        st.info("è¯·ç¡®ä¿å·²å®‰è£…spacyå’Œè‹±æ–‡æ¨¡å‹: python -m spacy download en_core_web_sm")
        st.stop()

# åŠ è½½æ•°æ®
@st.cache_data
def load_data():
    try:
        df = pd.read_csv('real_python_courses_analysis.csv')
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_columns = ['Title', 'Content', 'Date']
        if not all(col in df.columns for col in required_columns):
            st.error(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")
            st.stop()
        
        # æ¸…ç†æ•°æ®
        df = df.dropna(subset=['Content', 'Date'])
        df = df[df['Content'].str.len() > 100]  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
        
        # å¤„ç†æ—¥æœŸ
        try:
            df['Date'] = pd.to_datetime(df['Date'])
            df['Year'] = df['Date'].dt.year
            df['Month'] = df['Date'].dt.to_period('M')
        except:
            st.warning("æ—¥æœŸæ ¼å¼å¯èƒ½æœ‰é—®é¢˜ï¼Œéƒ¨åˆ†æ—¶é—´åºåˆ—åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
            df['Year'] = 2024  # é»˜è®¤å¹´ä»½
            df['Month'] = '2024-01'
        
        return df
    except FileNotFoundError:
        st.error("æœªæ‰¾åˆ° 'real_python_courses_analysis.csv' æ–‡ä»¶ï¼Œè¯·ç¡®ä¿æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹")
        st.stop()
    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        st.stop()

# æ–‡æœ¬é¢„å¤„ç†
@st.cache_data
def preprocess_text(text, _nlp, _stop_words, _lemmatizer):
    """é¢„å¤„ç†æ–‡æœ¬å†…å®¹"""
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    
    # ç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^a-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    # ä½¿ç”¨SpaCyè¿›è¡Œåˆ†è¯å’Œè¯å½¢è¿˜åŸ
    doc = _nlp(text)
    tokens = []
    
    for token in doc:
        if (token.is_alpha and 
            not token.is_stop and 
            token.text not in _stop_words and 
            len(token.text) > 2 and
            len(token.text) < 20):  # é¿å…å¾ˆé•¿çš„è¯
            tokens.append(token.lemma_)
    
    return " ".join(tokens)

# è®­ç»ƒLDAæ¨¡å‹
@st.cache_data
def train_lda_model(texts, n_topics=7, max_features=1000):
    """è®­ç»ƒLDAä¸»é¢˜æ¨¡å‹"""
    # TF-IDFå‘é‡åŒ–
    vectorizer = TfidfVectorizer(
        max_df=0.95, 
        min_df=2, 
        max_features=max_features,
        stop_words='english'
    )
    
    tfidf_matrix = vectorizer.fit_transform(texts)
    feature_names = vectorizer.get_feature_names_out()
    
    # è®­ç»ƒLDAæ¨¡å‹
    lda_model = LatentDirichletAllocation(
        n_components=n_topics,
        learning_method='online',
        random_state=42,
        max_iter=10,
        doc_topic_prior=0.1,
        topic_word_prior=0.01
    )
    
    lda_output = lda_model.fit_transform(tfidf_matrix)
    
    return lda_model, lda_output, vectorizer, tfidf_matrix, feature_names

# è·å–ä¸»é¢˜å…³é”®è¯
def get_top_words(model, feature_names, n_top_words=10):
    """è·å–æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯"""
    topic_words = {}
    for topic_idx, topic in enumerate(model.components_):
        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_features_ind]
        topic_words[f"ä¸»é¢˜ {topic_idx + 1}"] = top_words
    return topic_words

# åˆ›å»ºé¥¼å›¾
def create_pie_chart(data, title):
    """ä½¿ç”¨Matplotlibåˆ›å»ºé¥¼å›¾"""
    fig, ax = plt.subplots(figsize=(10, 8))
    colors = sns.color_palette("husl", len(data))
    
    wedges, texts, autotexts = ax.pie(
        data.values, 
        labels=data.index, 
        autopct='%1.1f%%',
        colors=colors,
        startangle=90
    )
    
    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
    
    # ç¾åŒ–æ–‡æœ¬
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
    
    plt.tight_layout()
    return fig

# åˆ›å»ºæŸ±çŠ¶å›¾
def create_bar_chart(data, title, xlabel, ylabel):
    """ä½¿ç”¨Seabornåˆ›å»ºæŸ±çŠ¶å›¾"""
    fig, ax = plt.subplots(figsize=(12, 6))
    
    sns.barplot(data=data.reset_index(), x=data.index, y=data.values, ax=ax)
    
    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel(xlabel, fontsize=12)
    ax.set_ylabel(ylabel, fontsize=12)
    
    # æ—‹è½¬xè½´æ ‡ç­¾
    plt.xticks(rotation=45, ha='right')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(data.values):
        ax.text(i, v + 0.1, str(v), ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    return fig

# åˆ›å»ºæ—¶é—´è¶‹åŠ¿å›¾
def create_trend_chart(data, title):
    """åˆ›å»ºæ—¶é—´è¶‹åŠ¿å›¾"""
    fig, ax = plt.subplots(figsize=(14, 8))
    
    # ä¸ºæ¯ä¸ªä¸»é¢˜åˆ›å»ºä¸åŒçš„çº¿æ¡
    topics = data['ä¸»é¢˜åç§°'].unique()
    colors = sns.color_palette("husl", len(topics))
    
    for i, topic in enumerate(topics):
        topic_data = data[data['ä¸»é¢˜åç§°'] == topic]
        ax.plot(topic_data['Year'], topic_data['æ–‡ç« æ•°é‡'], 
               marker='o', label=topic, color=colors[i], linewidth=2, markersize=6)
    
    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('å¹´ä»½', fontsize=12)
    ax.set_ylabel('æ–‡ç« æ•°é‡', fontsize=12)
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

# åˆ›å»ºçƒ­åŠ›å›¾
def create_heatmap(data, title):
    """åˆ›å»ºç›¸å…³æ€§çƒ­åŠ›å›¾"""
    fig, ax = plt.subplots(figsize=(10, 8))
    
    sns.heatmap(data, annot=True, cmap='viridis', center=0, 
                square=True, linewidths=0.5, ax=ax)
    
    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
    plt.tight_layout()
    return fig

# ä¸»å‡½æ•°
def main():
    st.title("ğŸ“š Real Python åšå®¢å†…å®¹ä¸»é¢˜åˆ†æä»ªè¡¨ç›˜")
    st.markdown("**ä½¿ç”¨ Seaborn + Matplotlib å¯è§†åŒ–æ–¹æ¡ˆ**")
    st.markdown("---")
    
    # åŠ è½½èµ„æº
    with st.spinner("æ­£åœ¨åŠ è½½NLPèµ„æº..."):
        nlp, stop_words, lemmatizer = load_nlp_resources()
    
    # åŠ è½½æ•°æ®
    with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®..."):
        df = load_data()
    
    st.success(f"âœ… æˆåŠŸåŠ è½½ {len(df)} ç¯‡æ–‡ç« ")
    
    # ä¾§è¾¹æ é…ç½®
    st.sidebar.header("ğŸ“Š åˆ†æè®¾ç½®")
    
    # ä¸»é¢˜æ•°é‡é€‰æ‹©
    n_topics = st.sidebar.slider(
        "é€‰æ‹©ä¸»é¢˜æ•°é‡ (LDA)", 
        min_value=3, 
        max_value=15, 
        value=7,
        help="é€‰æ‹©è¦å‘ç°çš„ä¸»é¢˜æ•°é‡"
    )
    
    # æœ€å¤§ç‰¹å¾è¯æ•°é‡
    max_features = st.sidebar.slider(
        "æœ€å¤§ç‰¹å¾è¯æ•°é‡", 
        min_value=500, 
        max_value=2000, 
        value=1000,
        help="ç”¨äºä¸»é¢˜å»ºæ¨¡çš„æœ€å¤§è¯æ±‡æ•°é‡"
    )
    
    # é¢„å¤„ç†æ–‡æœ¬
    if 'cleaned_content' not in df.columns:
        with st.spinner("æ­£åœ¨é¢„å¤„ç†æ–‡æœ¬..."):
            progress_bar = st.progress(0)
            cleaned_texts = []
            
            for i, content in enumerate(df['Content']):
                cleaned_text = preprocess_text(content, nlp, stop_words, lemmatizer)
                cleaned_texts.append(cleaned_text)
                progress_bar.progress((i + 1) / len(df))
            
            df['cleaned_content'] = cleaned_texts
            progress_bar.empty()
    
    # è¿‡æ»¤æ‰æ¸…ç†åå†…å®¹å¤ªçŸ­çš„æ–‡ç« 
    df = df[df['cleaned_content'].str.len() > 20]
    
    if len(df) == 0:
        st.error("é¢„å¤„ç†åæ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹")
        st.stop()
    
    # è®­ç»ƒLDAæ¨¡å‹
    with st.spinner("æ­£åœ¨è®­ç»ƒä¸»é¢˜æ¨¡å‹..."):
        lda_model, lda_output, vectorizer, tfidf_matrix, feature_names = train_lda_model(
            df['cleaned_content'], n_topics, max_features
        )
    
    # ä¸ºæ¯ç¯‡æ–‡ç« åˆ†é…ä¸»å¯¼ä¸»é¢˜
    df['dominant_topic'] = lda_output.argmax(axis=1)
    df['topic_probability'] = lda_output.max(axis=1)
    
    # è·å–ä¸»é¢˜å…³é”®è¯
    top_topic_words = get_top_words(lda_model, feature_names, 10)
    
    st.success("âœ… ä¸»é¢˜æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    # === ä¸»é¢˜æ¦‚è§ˆ ===
    st.header("ğŸ¯ ä¸»é¢˜æ¦‚è§ˆ")
    
    # æ˜¾ç¤ºä¸»é¢˜å…³é”®è¯
    cols = st.columns(3)
    topics_list = list(top_topic_words.keys())
    
    for i in range(min(3, len(topics_list))):
        with cols[i]:
            topic_name = topics_list[i]
            words = top_topic_words[topic_name]
            st.subheader(f"**{topic_name}**")
            st.write("**å…³é”®è¯:**")
            for j, word in enumerate(words[:5]):
                st.write(f"{j+1}. {word}")
    
    # ä¾§è¾¹æ æ˜¾ç¤ºæ‰€æœ‰ä¸»é¢˜
    st.sidebar.subheader("ğŸ” æ‰€æœ‰ä¸»é¢˜å…³é”®è¯")
    selected_topic_display = st.sidebar.selectbox(
        "é€‰æ‹©æŸ¥çœ‹å…·ä½“ä¸»é¢˜",
        options=list(top_topic_words.keys())
    )
    
    with st.sidebar.expander(f"ğŸ“ {selected_topic_display} è¯¦ç»†ä¿¡æ¯"):
        words = top_topic_words[selected_topic_display]
        for i, word in enumerate(words):
            st.write(f"{i+1}. **{word}**")
    
    st.markdown("---")
    
    # === ä¸»é¢˜åˆ†å¸ƒåˆ†æ ===
    st.header("ğŸ“Š ä¸»é¢˜åˆ†å¸ƒåˆ†æ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ä¸»é¢˜åˆ†å¸ƒé¥¼å›¾
        topic_counts = df['dominant_topic'].value_counts()
        topic_counts.index = [f"ä¸»é¢˜ {i + 1}" for i in topic_counts.index]
        
        st.subheader("å„ä¸»é¢˜æ–‡ç« æ•°é‡åˆ†å¸ƒ")
        fig_pie = create_pie_chart(topic_counts, "å„ä¸»é¢˜æ–‡ç« æ•°é‡åˆ†å¸ƒ")
        st.pyplot(fig_pie)
    
    with col2:
        # ä¸»é¢˜åˆ†å¸ƒæŸ±çŠ¶å›¾
        st.subheader("å„ä¸»é¢˜æ–‡ç« æ•°é‡ç»Ÿè®¡")
        fig_bar = create_bar_chart(topic_counts, "å„ä¸»é¢˜æ–‡ç« æ•°é‡ç»Ÿè®¡", "ä¸»é¢˜", "æ–‡ç« æ•°é‡")
        st.pyplot(fig_bar)
    
    # ä¸»é¢˜ç½®ä¿¡åº¦åˆ†æ
    st.subheader("ğŸ¯ ä¸»é¢˜ç½®ä¿¡åº¦åˆ†æ")
    
    # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
    avg_confidence = df.groupby('dominant_topic')['topic_probability'].mean()
    avg_confidence.index = [f"ä¸»é¢˜ {i + 1}" for i in avg_confidence.index]
    
    fig_conf = create_bar_chart(avg_confidence, "å„ä¸»é¢˜çš„å¹³å‡ç½®ä¿¡åº¦", "ä¸»é¢˜", "å¹³å‡ç½®ä¿¡åº¦")
    st.pyplot(fig_conf)
    
    st.markdown("---")
    
    # === æ—¶é—´è¶‹åŠ¿åˆ†æ ===
    if 'Year' in df.columns and df['Year'].nunique() > 1:
        st.header("ğŸ“ˆ ä¸»é¢˜æ—¶é—´è¶‹åŠ¿åˆ†æ")
        
        # æŒ‰å¹´ä»½å’Œä¸»é¢˜åˆ†ç»„
        yearly_trends = df.groupby(['Year', 'dominant_topic']).size().reset_index(name='æ–‡ç« æ•°é‡')
        yearly_trends['ä¸»é¢˜åç§°'] = yearly_trends['dominant_topic'].apply(lambda x: f"ä¸»é¢˜ {x + 1}")
        
        fig_trend = create_trend_chart(yearly_trends, "å„ä¸»é¢˜æ–‡ç« å‘å¸ƒè¶‹åŠ¿ï¼ˆæŒ‰å¹´ï¼‰")
        st.pyplot(fig_trend)
        
        st.markdown("---")
    
    # === ä¸»é¢˜è¯¦ç»†åˆ†æ ===
    st.header("ğŸ” ä¸»é¢˜è¯¦ç»†åˆ†æ")
    
    # é€‰æ‹©ä¸»é¢˜è¿›è¡Œè¯¦ç»†æŸ¥çœ‹
    selected_topic_id = st.selectbox(
        "é€‰æ‹©ä¸»é¢˜è¿›è¡Œè¯¦ç»†åˆ†æ",
        options=sorted(df['dominant_topic'].unique()),
        format_func=lambda x: f"ä¸»é¢˜ {x + 1}: {', '.join(top_topic_words[f'ä¸»é¢˜ {x + 1}'][:3])}"
    )
    
    # è¿‡æ»¤è¯¥ä¸»é¢˜çš„æ–‡ç« 
    topic_articles = df[df['dominant_topic'] == selected_topic_id].sort_values(
        by='topic_probability', ascending=False
    )
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader(f"ğŸ¯ ä¸»é¢˜ {selected_topic_id + 1} çš„æ–‡ç« åˆ—è¡¨ ({len(topic_articles)} ç¯‡)")
        
        # æ˜¾ç¤ºæ–‡ç« åˆ—è¡¨
        for idx, row in topic_articles.head(10).iterrows():
            with st.expander(f"ğŸ“„ {row['Title']} (ç½®ä¿¡åº¦: {row['topic_probability']:.2f})"):
                st.write(f"**å‘å¸ƒæ—¥æœŸ:** {row['Date']}")
                if 'URL' in row:
                    st.write(f"**é“¾æ¥:** {row['URL']}")
                
                # æ˜¾ç¤ºæ–‡ç« æ‘˜è¦ï¼ˆå‰200ä¸ªå­—ç¬¦ï¼‰
                content_preview = row['Content'][:200] + "..." if len(row['Content']) > 200 else row['Content']
                st.write(f"**å†…å®¹é¢„è§ˆ:** {content_preview}")
    
    with col2:
        # è¯¥ä¸»é¢˜çš„è¯äº‘
        st.subheader(f"â˜ï¸ ä¸»é¢˜ {selected_topic_id + 1} è¯äº‘")
        
        # è·å–è¯¥ä¸»é¢˜çš„å…³é”®è¯å’Œæƒé‡
        topic_words = top_topic_words[f"ä¸»é¢˜ {selected_topic_id + 1}"]
        word_freq = {word: len(topic_words) - i for i, word in enumerate(topic_words)}
        
        try:
            wordcloud = WordCloud(
                width=400, 
                height=300, 
                background_color='white',
                colormap='viridis',
                max_words=50
            ).generate_from_frequencies(word_freq)
            
            fig_wc, ax_wc = plt.subplots(figsize=(8, 6))
            ax_wc.imshow(wordcloud, interpolation='bilinear')
            ax_wc.axis("off")
            ax_wc.set_title(f"ä¸»é¢˜ {selected_topic_id + 1} è¯äº‘", fontsize=14, fontweight='bold')
            st.pyplot(fig_wc)
        except Exception as e:
            st.error(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {e}")
    
    st.markdown("---")
    
    # === ç»Ÿè®¡ä¿¡æ¯ ===
    st.header("ğŸ“‹ æ•°æ®ç»Ÿè®¡ä¿¡æ¯")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("æ€»æ–‡ç« æ•°", len(df))
    
    with col2:
        st.metric("ä¸»é¢˜æ•°é‡", n_topics)
    
    with col3:
        st.metric("è¯æ±‡è¡¨å¤§å°", len(feature_names))
    
    with col4:
        avg_confidence = df['topic_probability'].mean()
        st.metric("å¹³å‡ä¸»é¢˜ç½®ä¿¡åº¦", f"{avg_confidence:.3f}")
    
    # å…³é”®è¯ç»Ÿè®¡
    st.subheader("ğŸ“ é«˜é¢‘å…³é”®è¯ç»Ÿè®¡")
    
    # è·å–æ‰€æœ‰æ¸…ç†åçš„æ–‡æœ¬
    all_words = []
    for text in df['cleaned_content']:
        all_words.extend(text.split())
    
    # ç»Ÿè®¡è¯é¢‘
    word_freq = Counter(all_words)
    top_words = word_freq.most_common(20)
    
    word_df = pd.DataFrame(top_words, columns=['è¯æ±‡', 'é¢‘æ¬¡'])
    
    fig_words = create_bar_chart(word_df.set_index('è¯æ±‡')['é¢‘æ¬¡'], 
                                "é«˜é¢‘è¯æ±‡æ’è¡Œæ¦œ (Top 20)", "è¯æ±‡", "é¢‘æ¬¡")
    st.pyplot(fig_words)
    
    st.markdown("---")
    
    # === ä¸‹è½½ç»“æœ ===
    st.header("ğŸ’¾ ä¸‹è½½åˆ†æç»“æœ")
    
    # å‡†å¤‡ä¸‹è½½æ•°æ®
    download_df = df[['Title', 'Date', 'dominant_topic', 'topic_probability']].copy()
    download_df['ä¸»é¢˜åç§°'] = download_df['dominant_topic'].apply(lambda x: f"ä¸»é¢˜ {x + 1}")
    download_df['ä¸»é¢˜å…³é”®è¯'] = download_df['dominant_topic'].apply(
        lambda x: ', '.join(top_topic_words[f"ä¸»é¢˜ {x + 1}"][:5])
    )
    
    csv = download_df.to_csv(index=False, encoding='utf-8-sig')
    
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½ä¸»é¢˜åˆ†æç»“æœ (CSV)",
        data=csv,
        file_name="real_python_topic_analysis_results.csv",
        mime="text/csv"
    )
    
    # æ˜¾ç¤ºç»“æœé¢„è§ˆ
    with st.expander("ğŸ“Š æŸ¥çœ‹åˆ†æç»“æœé¢„è§ˆ"):
        st.dataframe(download_df, use_container_width=True)

if __name__ == "__main__":
    main() 