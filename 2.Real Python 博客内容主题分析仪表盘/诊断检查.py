#!/usr/bin/env python3
"""
Real Python åšå®¢å†…å®¹ä¸»é¢˜åˆ†æä»ªè¡¨ç›˜ - è¯Šæ–­æ£€æŸ¥è„šæœ¬
æ£€æŸ¥ç¯å¢ƒé…ç½®å’Œä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…
"""

import sys
import os
import subprocess
from pathlib import Path

def print_header(title):
    print("\n" + "="*50)
    print(f"ğŸ” {title}")
    print("="*50)

def check_python_version():
    print_header("Python ç‰ˆæœ¬æ£€æŸ¥")
    version = sys.version_info
    print(f"Python ç‰ˆæœ¬: {version.major}.{version.minor}.{version.micro}")
    
    if version >= (3, 8):
        print("âœ… Python ç‰ˆæœ¬ç¬¦åˆè¦æ±‚ (>= 3.8)")
        return True
    else:
        print("âŒ Python ç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦ 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬")
        return False

def check_virtual_env():
    print_header("è™šæ‹Ÿç¯å¢ƒæ£€æŸ¥")
    
    # æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒæ–‡ä»¶å¤¹
    venv_path = Path("venv_topic_analyzer")
    if venv_path.exists():
        print("âœ… æ‰¾åˆ°è™šæ‹Ÿç¯å¢ƒæ–‡ä»¶å¤¹")
        
        # æ£€æŸ¥æ¿€æ´»è„šæœ¬
        activate_script = venv_path / "Scripts" / "activate.bat"
        if activate_script.exists():
            print("âœ… è™šæ‹Ÿç¯å¢ƒæ¿€æ´»è„šæœ¬å­˜åœ¨")
            return True
        else:
            print("âŒ è™šæ‹Ÿç¯å¢ƒæ¿€æ´»è„šæœ¬ç¼ºå¤±")
            return False
    else:
        print("âŒ è™šæ‹Ÿç¯å¢ƒæ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        return False

def check_data_file():
    print_header("æ•°æ®æ–‡ä»¶æ£€æŸ¥")
    
    data_file = Path("real_python_courses_analysis.csv")
    if data_file.exists():
        print("âœ… æ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        size = data_file.stat().st_size
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {size:,} å­—èŠ‚")
        
        if size > 0:
            print("âœ… æ•°æ®æ–‡ä»¶ä¸ä¸ºç©º")
            return True
        else:
            print("âŒ æ•°æ®æ–‡ä»¶ä¸ºç©º")
            return False
    else:
        print("âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: real_python_courses_analysis.csv")
        return False

def check_dependencies():
    print_header("ä¾èµ–åŒ…æ£€æŸ¥")
    
    required_packages = [
        ('streamlit', 'streamlit'),
        ('pandas', 'pandas'),
        ('numpy', 'numpy'),
        ('scikit-learn', 'sklearn'),
        ('nltk', 'nltk'),
        ('spacy', 'spacy'),
        ('plotly', 'plotly'),
        ('seaborn', 'seaborn'),
        ('matplotlib', 'matplotlib'),
        ('wordcloud', 'wordcloud')
    ]
    
    all_good = True
    
    for package_name, import_name in required_packages:
        try:
            exec(f"import {import_name}")
            print(f"âœ… {package_name}")
        except ImportError as e:
            print(f"âŒ {package_name} - æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥: {e}")
            all_good = False
    
    return all_good

def check_spacy_model():
    print_header("SpaCy è‹±æ–‡æ¨¡å‹æ£€æŸ¥")
    
    try:
        import spacy
        nlp = spacy.load("en_core_web_sm")
        print("âœ… SpaCy è‹±æ–‡æ¨¡å‹ (en_core_web_sm) å·²å®‰è£…")
        return True
    except OSError:
        print("âŒ SpaCy è‹±æ–‡æ¨¡å‹æœªå®‰è£…")
        print("ğŸ’¡ è¯·è¿è¡Œ: python -m spacy download en_core_web_sm")
        return False

def check_nltk_data():
    print_header("NLTK æ•°æ®æ£€æŸ¥")
    
    try:
        import nltk
        from nltk.corpus import stopwords
        from nltk.stem import WordNetLemmatizer
        
        # å°è¯•åŠ è½½åœç”¨è¯
        stop_words = stopwords.words('english')
        lemmatizer = WordNetLemmatizer()
        
        print("âœ… NLTK åœç”¨è¯å’Œè¯å½¢è¿˜åŸå™¨å¯ç”¨")
        return True
    except Exception as e:
        print(f"âŒ NLTK æ•°æ®ç¼ºå¤±: {e}")
        print("ğŸ’¡ è¯·è¿è¡Œåº”ç”¨æ—¶ä¼šè‡ªåŠ¨ä¸‹è½½ï¼Œæˆ–æ‰‹åŠ¨è¿è¡Œ:")
        print("   import nltk; nltk.download('stopwords'); nltk.download('wordnet')")
        return False

def run_quick_test():
    print_header("å¿«é€ŸåŠŸèƒ½æµ‹è¯•")
    
    try:
        # æµ‹è¯•ä¸»è¦å¯¼å…¥
        import pandas as pd
        import streamlit as st
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.decomposition import LatentDirichletAllocation
        
        # æµ‹è¯•ç®€å•åŠŸèƒ½
        sample_data = pd.DataFrame({'text': ['hello world', 'python programming']})
        vectorizer = TfidfVectorizer(max_features=10)
        matrix = vectorizer.fit_transform(sample_data['text'])
        
        print("âœ… æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        print(f"âŒ åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    print("ğŸ¯ Real Python åšå®¢å†…å®¹ä¸»é¢˜åˆ†æä»ªè¡¨ç›˜ - ç¯å¢ƒè¯Šæ–­")
    print("ğŸ” æ­£åœ¨æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒå’Œä¾èµ–é…ç½®...")
    
    checks = [
        ("Python ç‰ˆæœ¬", check_python_version),
        ("è™šæ‹Ÿç¯å¢ƒ", check_virtual_env),
        ("æ•°æ®æ–‡ä»¶", check_data_file),
        ("ä¾èµ–åŒ…", check_dependencies),
        ("SpaCy æ¨¡å‹", check_spacy_model),
        ("NLTK æ•°æ®", check_nltk_data),
        ("åŠŸèƒ½æµ‹è¯•", run_quick_test)
    ]
    
    results = []
    for name, check_func in checks:
        try:
            result = check_func()
            results.append((name, result))
        except Exception as e:
            print(f"âŒ {name} æ£€æŸ¥æ—¶å‡ºé”™: {e}")
            results.append((name, False))
    
    # æ€»ç»“æŠ¥å‘Š
    print_header("æ£€æŸ¥ç»“æœæ€»ç»“")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{name:.<20} {status}")
    
    print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed}/{total} é¡¹æ£€æŸ¥é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼æ‚¨å¯ä»¥æ­£å¸¸è¿è¡Œåº”ç”¨ã€‚")
        print("ğŸ’¡ è¿è¡Œå‘½ä»¤: streamlit run topic_dashboard_enhanced.py")
    else:
        print(f"\nâš ï¸  æœ‰ {total - passed} é¡¹æ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·æ ¹æ®ä¸Šè¿°æç¤ºè¿›è¡Œä¿®å¤ã€‚")
    
    print("\n" + "="*50)

if __name__ == "__main__":
    main() 